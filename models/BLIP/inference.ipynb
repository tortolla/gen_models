{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf37aa2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from src.model import (\n",
    "    load_blip_captioning,\n",
    "    load_blip_vqa,\n",
    "    load_blip_retrieval,\n",
    ")\n",
    "\n",
    "caption_model, caption_processor, device = load_blip_captioning()\n",
    "vqa_model, vqa_processor, _ = load_blip_vqa(device)\n",
    "retrieval_model, retrieval_processor, _ = load_blip_retrieval(device)\n",
    "\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e302a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "image_paths = [\n",
    "    DATA_DIR / \"cat.jpeg\",\n",
    "    DATA_DIR / \"dog.jpeg\",\n",
    "    DATA_DIR / \"girl_cat.jpeg\",\n",
    "]\n",
    "\n",
    "for p in image_paths:\n",
    "    print(p, \"exists:\", p.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4619a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for img_path in image_paths:\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = caption_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = caption_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=30,\n",
    "        )\n",
    "\n",
    "    caption = caption_processor.decode(\n",
    "        output_ids[0],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    print(f\"{img_path.name}: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1b391",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "questions_per_image = {\n",
    "    \"cat.jpeg\": [\n",
    "        \"What animal is in the picture?\",\n",
    "    ],\n",
    "    \"dog.jpeg\": [\n",
    "        \"What animal is in the picture?\",\n",
    "        \"Is the animal sitting on grass?\",\n",
    "    ],\n",
    "    \"girl_cat.jpeg\": [\n",
    "        \"What animal is the girl holding?\",\n",
    "        \"Is the girl smiling?\",\n",
    "        \"Is this photo taken indoors?\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "for img_path in image_paths:\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    name = img_path.name\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "\n",
    "    for question in questions_per_image.get(name, []):\n",
    "        inputs = vqa_processor(\n",
    "            images=img,\n",
    "            text=question,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = vqa_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=16,\n",
    "            )\n",
    "\n",
    "        answer = vqa_processor.decode(\n",
    "            output_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da055763",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "candidate_texts = [\n",
    "    \"A tabby cat sitting outside.\",\n",
    "    \"A golden retriever puppy lying on the grass.\",\n",
    "    \"A young woman holding a gray cat in her arms.\",\n",
    "]\n",
    "\n",
    "for img_path in image_paths:\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    print(f\"\\n=== {img_path.name} ===\")\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for text in candidate_texts:\n",
    "        inputs = retrieval_processor(\n",
    "            images=img,\n",
    "            text=text,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # itm_scores: чем выше, тем лучше матч\n",
    "            itm_scores = retrieval_model(**inputs)[0]          # ITM head\n",
    "            cosine_score = retrieval_model(\n",
    "                **inputs, use_itm_head=False\n",
    "            )[0]                                              # cosine similarity\n",
    "\n",
    "        score = float(itm_scores.squeeze().cpu())\n",
    "        cos = float(cosine_score.squeeze().cpu())\n",
    "        scores.append((text, score, cos))\n",
    "\n",
    "    # сортируем по itm score\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for text, s, cos in scores:\n",
    "        print(f\"'{text}'  ->  ITM Score={s:.3f}, Cosine={cos:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
